{
 "cells": [
  {
   "source": [
    "# Maciej Legas 2031545 Coursework 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools as itr\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic I/O functions\n",
    "Below are three functions for reading the dataset, storing evaluation results and finding unique values in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_dataset(data_file):\n",
    "    filtered_dataset = []\n",
    "    \n",
    "    with open(data_file) as data:\n",
    "        read_data = csv.reader(data)\n",
    "        for row in read_data:\n",
    "            # The row below removes empty values\n",
    "            filtered_data = list(filter(None, row))\n",
    "            filtered_dataset.append(filtered_data)\n",
    "        \n",
    "    return filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_results(results):\n",
    "    results_file = open(\"results.txt\", \"w\")\n",
    "    results_file.write(\"\\n\".join(str(row) for row in results))\n",
    "    results_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_values(dataset):\n",
    "    unique_list = []\n",
    "\n",
    "    for row in dataset:\n",
    "        for value in row:\n",
    "            if value not in unique_list:\n",
    "                unique_list.append(value)\n",
    "\n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = read_dataset('GroceryStore.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent Itemset\n",
    "\n",
    "Brute-force approach, no pruning. Computationally expensive as we are considering all possible itemset combinations as candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequent_itemset(dataset, min_sup, itemset_limit=5):\n",
    "    itemsets = []\n",
    "    # Dictionary: Itemset as key - Support as value\n",
    "    supports = {}\n",
    "    # Dictionary: Itemset as key - Frequency as value\n",
    "    frequencies = {}\n",
    "    transaction_count = len(dataset)\n",
    "\n",
    "    # Find unique values (1-itemsets)\n",
    "    unique_list = unique_values(dataset)\n",
    "\n",
    "    if itemset_limit < 0 or itemset_limit > len(unique_list):\n",
    "        print(\"Illegal value set as the itemset limit, returning to the default behaviour.\")\n",
    "        itemset_limit = 0\n",
    "\n",
    "    if itemset_limit == 0:\n",
    "        itemset_limit = len(unique_list)\n",
    "\n",
    "    else:\n",
    "        itemset_limit = itemset_limit + 1\n",
    "\n",
    "    for i in range(1, itemset_limit):\n",
    "        # Use itertools to find all possible combinations of the unique values without repetitions (order does not matter as well), with i being the value of r\n",
    "        combinations = itr.combinations(unique_list, i)\n",
    "        itemset = []\n",
    "\n",
    "        for combination in combinations:        \n",
    "            appearance_count = 0\n",
    "\n",
    "            for row in dataset:\n",
    "                # Check if a row of the given dataset contains all values of the combination\n",
    "                if all(value in row for value in combination):\n",
    "                    appearance_count = appearance_count + 1\n",
    "\n",
    "            support = appearance_count / transaction_count\n",
    "\n",
    "            # Store where an itemset appears in the dataset\n",
    "            frequencies[combination] = appearance_count\n",
    "\n",
    "            if support >= min_sup:\n",
    "                # Store rows in which the combination appears\n",
    "                itemset.append(combination)\n",
    "                # Store the supports of an itemset\n",
    "                supports[combination] = support\n",
    "\n",
    "        itemset.sort()\n",
    "        itemsets.append(itemset)\n",
    "\n",
    "    return frequencies, itemsets, supports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "frequencies, itemsets, supports = frequent_itemset(dataset=data, min_sup=0.1, itemset_limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_itemsets(itemsets, supports):\n",
    "    count = 1\n",
    "    print(\"--------------------------------------\")\n",
    "    for itemset_row in itemsets:\n",
    "        if not itemset_row:\n",
    "            print(\"No {}-itemsets have been found.\".format(count))\n",
    "            print(\"Try to lower the minimum support value if that was not intended.\")\n",
    "        else:\n",
    "            print(\"Possible {}-itemsets with supports:\".format(count))\n",
    "            for itemset in itemset_row:\n",
    "                print(itemset, supports[itemset])\n",
    "        print(\"--------------------------------------\")\n",
    "        count = count + 1"
   ]
  },
  {
   "source": [
    "Below are the found 2, 3, 4 and 5-frequent itemsets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--------------------------------------\nPossible 1-itemsets with supports:\n('Bread',) 0.43780935653840014\n('Butter',) 0.4375698547022194\n('Cheese',) 0.43717068497525147\n('Coffee Powder',) 0.4398052051732397\n('Ghee',) 0.43988503911863325\n('Lassi',) 0.43365799137793387\n('Milk',) 0.44116238224493054\n('Panner',) 0.4346159987226569\n('Sugar',) 0.43764968864761294\n('Sweet',) 0.43772952259300657\n('Tea Powder',) 0.4297461280536484\n('Yougurt',) 0.4393262015008782\n--------------------------------------\nPossible 2-itemsets with supports:\n('Bread', 'Milk') 0.20094204055564427\n('Bread', 'Sugar') 0.19790835063068818\n('Bread', 'Sweet') 0.20269838735430304\n('Butter', 'Bread') 0.1969503432859652\n('Butter', 'Cheese') 0.19838735430304966\n('Butter', 'Ghee') 0.20197988184576082\n('Butter', 'Milk') 0.19886635797541113\n('Butter', 'Panner') 0.1982276864122625\n('Butter', 'Sugar') 0.20525307360689765\n('Butter', 'Sweet') 0.20301772313587738\n('Butter', 'Tea Powder') 0.19295864601628612\n('Butter', 'Yougurt') 0.20190004790036722\n('Cheese', 'Bread') 0.20197988184576082\n('Cheese', 'Milk') 0.19703017723135877\n('Cheese', 'Panner') 0.19934536164777264\n('Cheese', 'Sugar') 0.19639150566821012\n('Cheese', 'Sweet') 0.1994251955931662\n('Cheese', 'Tea Powder') 0.19088296343605302\n('Coffee Powder', 'Bread') 0.20182021395497365\n('Coffee Powder', 'Butter') 0.19974453137474055\n('Coffee Powder', 'Cheese') 0.20094204055564427\n('Coffee Powder', 'Ghee') 0.20581191122465273\n('Coffee Powder', 'Milk') 0.20102187450103784\n('Coffee Powder', 'Panner') 0.19814785246686892\n('Coffee Powder', 'Sugar') 0.19982436532013412\n('Coffee Powder', 'Sweet') 0.19926552770237904\n('Coffee Powder', 'Tea Powder') 0.1956730001596679\n('Coffee Powder', 'Yougurt') 0.20397573048060036\n('Ghee', 'Bread') 0.19982436532013412\n('Ghee', 'Cheese') 0.1973495130129331\n('Ghee', 'Milk') 0.20046303688328276\n('Ghee', 'Panner') 0.20142104422800575\n('Ghee', 'Sugar') 0.20086220661025067\n('Ghee', 'Sweet') 0.19990419926552772\n('Ghee', 'Tea Powder') 0.1930384799616797\n('Lassi', 'Bread') 0.20006386715631486\n('Lassi', 'Butter') 0.19966469742934695\n('Lassi', 'Cheese') 0.1983075203576561\n('Lassi', 'Coffee Powder') 0.20054287082867636\n('Lassi', 'Ghee') 0.20046303688328276\n('Lassi', 'Milk') 0.20269838735430304\n('Lassi', 'Panner') 0.19934536164777264\n('Lassi', 'Sugar') 0.19982436532013412\n('Lassi', 'Sweet') 0.20565224333386556\n('Lassi', 'Tea Powder') 0.19471499281494492\n('Lassi', 'Yougurt') 0.1968705093405716\n('Panner', 'Bread') 0.20357656075363245\n('Panner', 'Milk') 0.1982276864122625\n('Panner', 'Sugar') 0.19998403321092129\n('Panner', 'Sweet') 0.19998403321092129\n('Sugar', 'Milk') 0.204614402043749\n('Sugar', 'Sweet') 0.19918569375698547\n('Sweet', 'Milk') 0.20054287082867636\n('Tea Powder', 'Bread') 0.1951939964873064\n('Tea Powder', 'Milk') 0.19511416254191283\n('Tea Powder', 'Panner') 0.19231997445313748\n('Tea Powder', 'Sugar') 0.19742934695832667\n('Tea Powder', 'Sweet') 0.19982436532013412\n('Yougurt', 'Bread') 0.20014370110170845\n('Yougurt', 'Cheese') 0.202139549736548\n('Yougurt', 'Ghee') 0.197748682739901\n('Yougurt', 'Milk') 0.20062270477406993\n('Yougurt', 'Panner') 0.1994251955931662\n('Yougurt', 'Sugar') 0.20190004790036722\n('Yougurt', 'Sweet') 0.19782851668529458\n('Yougurt', 'Tea Powder') 0.19846718824844323\n--------------------------------------\nPossible 3-itemsets with supports:\n('Lassi', 'Panner', 'Sweet') 0.10098994092288041\n--------------------------------------\nNo 4-itemsets have been found.\nTry to lower the minimum support value if that was not intended.\n--------------------------------------\nNo 5-itemsets have been found.\nTry to lower the minimum support value if that was not intended.\n--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_itemsets(itemsets, supports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding associated rules\n",
    "This method has been optimized to use stored frequencies in calculating confidence, running way faster than searching each time for set intersections for each X and Y in the dataset.\n",
    "\n",
    "The below two functions have been implemented in order to help with generating candidate rules of appropriate length. By this, we mean that there is always a set of X and Y and not above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper recursive function adapted and modified from source: https://stackoverflow.com/questions/19368375/set-partitions-in-python\n",
    "def partition(itemset):\n",
    "    if len(itemset) == 1:\n",
    "        yield [ itemset ]\n",
    "        return\n",
    "\n",
    "    first = itemset[0]\n",
    "    for smaller in partition(itemset[1:]):\n",
    "        for n, subset in enumerate(smaller):\n",
    "            yield smaller[:n] + [[ first ] + subset]  + smaller[n+1:]\n",
    "        yield [[ first ]] + smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidate_rules(itemset):\n",
    "    candidate_rules = []\n",
    "\n",
    "    if len(itemset) <= 1:\n",
    "        return itemset\n",
    "\n",
    "    else:\n",
    "        partitioned_itemset = partition(itemset)\n",
    "    \n",
    "    for items in partitioned_itemset:\n",
    "        if len(items) != 2:\n",
    "            # For candidate rules we require to have all items in two subsets\n",
    "            # We are rejecting subsets with multiple single items such as ['A', 'B', 'CD']\n",
    "            continue\n",
    "        else:\n",
    "            candidate_rules.append(items)\n",
    "            # Reverse the order for reverse rules, for example: A -> BC, BC -> A\n",
    "            candidate_rules.append(items[::-1])\n",
    "    \n",
    "    return candidate_rules"
   ]
  },
  {
   "source": [
    "Test showcase of how the functions above work."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['A'], ['B', 'C', 'D']]\n[['B', 'C', 'D'], ['A']]\n[['A', 'B'], ['C', 'D']]\n[['C', 'D'], ['A', 'B']]\n[['B'], ['A', 'C', 'D']]\n[['A', 'C', 'D'], ['B']]\n[['A', 'B', 'C'], ['D']]\n[['D'], ['A', 'B', 'C']]\n[['B', 'C'], ['A', 'D']]\n[['A', 'D'], ['B', 'C']]\n[['A', 'C'], ['B', 'D']]\n[['B', 'D'], ['A', 'C']]\n[['C'], ['A', 'B', 'D']]\n[['A', 'B', 'D'], ['C']]\n"
     ]
    }
   ],
   "source": [
    "test_list = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "for value in generate_candidate_rules(test_list):\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_association_rules(dataset, frequencies, itemsets, supports, min_conf):\n",
    "    # List for generated rules\n",
    "    output = [[], [], [], []]\n",
    "    confidences = []\n",
    "    transaction_count = len(dataset)\n",
    "    # Removing empty itemsets\n",
    "    filtered_itemsets = list(filter(None, itemsets))\n",
    "\n",
    "    # Iterate only through itemsets with a collection of 2 or above, no use of going through 1-itemsets\n",
    "    i = len(filtered_itemsets) - 1\n",
    "\n",
    "    while i > 0:\n",
    "        for itemset in filtered_itemsets[i]:\n",
    "            candidate_rules = generate_candidate_rules(list(itemset))\n",
    "            \n",
    "            for row in candidate_rules:\n",
    "                # Find where in the dataset both items appear\n",
    "                combined_x_and_y = row[0] + row[1]\n",
    "                key = combined_x_and_y\n",
    "\n",
    "                if tuple(key) not in frequencies:\n",
    "                    # Try to sort the key and see if we can find it before iterating through all the keys\n",
    "                    key.sort()\n",
    "                    if tuple(key) not in frequencies:\n",
    "                        # We try to find the key containing the values while being of the same length\n",
    "                        for dictionary_key in frequencies.keys():\n",
    "                            if all(value in dictionary_key for value in key) and len(dictionary_key) == len(key):\n",
    "                                key = dictionary_key\n",
    "\n",
    "                x_and_y_frequencies = frequencies[tuple(key)]\n",
    "                x_freq = frequencies[tuple(row[0])]\n",
    "\n",
    "                # Calculate confidence: X and Y frequencies / X frequency\n",
    "                confidence = x_and_y_frequencies / x_freq\n",
    "                if confidence >= min_conf:\n",
    "                    output[0].append(row[0])\n",
    "                    output[1].append(row[1])\n",
    "                    output[2].append(supports[itemset])\n",
    "                    output[3].append(confidence)\n",
    "\n",
    "        i = i - 1\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = mine_association_rules(data, frequencies, itemsets, supports, min_conf=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_association_rules(output):\n",
    "    if not output[0]:\n",
    "        print(\"No association rules have been found.\")\n",
    "    else:\n",
    "        print(\"Association rules found:\")\n",
    "        for row in range(len(output[0])):\n",
    "            print(\"{} => {} | Support: {} | Confidence: {}\".format(output[0][row], output[1][row], output[2][row], output[3][row]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Association rules found:\n['Panner', 'Sweet'] => ['Lassi'] | Support: 0.10098994092288041 | Confidence: 0.5049900199600799\n['Lassi', 'Panner'] => ['Sweet'] | Support: 0.10098994092288041 | Confidence: 0.5066079295154186\n"
     ]
    }
   ],
   "source": [
    "print_association_rules(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apriori algorithm\n",
    "Below is the implementation of the apriori algorithm. The main repeating functions have been separated from the main function to separate ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_apriori_candidates(itemsets, k):\n",
    "    candidate_set = []\n",
    "    # If we do not need to meet any prefix criteria, just generate a combination\n",
    "    if k - 2 < 0:\n",
    "        combinations = itr.combinations(itemsets, k+1)\n",
    "        for combination in combinations:\n",
    "            candidate_set.append(combination)\n",
    "    else:\n",
    "        count = 0\n",
    "        for item in itemsets:\n",
    "            for row in range(count, len(itemsets)):\n",
    "                if item == itemsets[row]:\n",
    "                    continue\n",
    "                else:\n",
    "                    prefix_count = 0\n",
    "                    merged_itemset = []\n",
    "                    \n",
    "                    for prefix in range(k-1):\n",
    "                        if item[prefix] == itemsets[row][prefix]:\n",
    "                            prefix_count = prefix_count + 1\n",
    "\n",
    "                    if prefix_count == k-1:\n",
    "                        for prefix in range(k-1):\n",
    "                            merged_itemset.append(item[prefix])\n",
    "                        merged_itemset.append(item[k-1])\n",
    "                        merged_itemset.append(itemsets[row][k-1])\n",
    "                        candidate_set.append(merged_itemset)\n",
    "                        \n",
    "            count = count + 1\n",
    "            \n",
    "    return candidate_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_apriori_candidates(candidate_set, supports, min_sup, k):\n",
    "    pruned_candidate_set = []\n",
    "\n",
    "    for candidate in candidate_set:\n",
    "        filtered_candidate_set = []\n",
    "        # Partition the generated candidate set\n",
    "        partitions = partition(list(candidate))\n",
    "        for subsets in partitions:\n",
    "            for subset in subsets:\n",
    "                # Leave only subsets of length K\n",
    "                if len(subset) == k:\n",
    "                    if subset not in filtered_candidate_set:    \n",
    "                        filtered_candidate_set.append(subset)\n",
    "\n",
    "        for sets in filtered_candidate_set:\n",
    "            for subset in sets:\n",
    "                if type(subset) is not tuple:\n",
    "                    subset = (subset,)\n",
    "                if supports[subset] >= min_sup:\n",
    "                    # Eliminate subsets of length K that are not above the minimum support criteria\n",
    "                    if sets not in pruned_candidate_set:\n",
    "                        pruned_candidate_set.append(sets)\n",
    "\n",
    "    return pruned_candidate_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidate_elimination_apriori(candidate_set, dataset, frequencies, supports, min_sup, k):\n",
    "    transaction_count = len(dataset)\n",
    "    itemset = []\n",
    "    \n",
    "    for candidate in candidate_set:\n",
    "        item_positions = []            \n",
    "        appearance_count = 0\n",
    "\n",
    "        # This row converts the tuple of tuples of strings into a single tuple of strings\n",
    "        # It is required to get the result from itertools.combinations properly iterated\n",
    "        if k == 1:\n",
    "            candidate = tuple(itr.chain(*candidate))\n",
    "\n",
    "        # Else just convert the list of candidates to a tuple\n",
    "        else:\n",
    "            candidate = tuple(candidate)\n",
    "\n",
    "        for row_count, row in enumerate(dataset):\n",
    "            # Check if a row of the given dataset contains all values of the candidate\n",
    "            if all(value in row for value in candidate):\n",
    "                appearance_count = appearance_count + 1\n",
    "                # Collect in which rows the candidate appears\n",
    "                item_positions.append(row_count)\n",
    "\n",
    "        support = appearance_count / transaction_count\n",
    "\n",
    "        # Store where a candidate appears in the dataset\n",
    "        frequencies[candidate] = appearance_count\n",
    "\n",
    "        if support > min_sup:\n",
    "            # Store the frequent candidate\n",
    "            itemset.append(candidate)\n",
    "            # Store the support of the itemset\n",
    "            supports[candidate] = support\n",
    "\n",
    "    itemset.sort()\n",
    "\n",
    "    return frequencies, itemset, supports"
   ]
  },
  {
   "source": [
    "Below is the main algorithm function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori(dataset, min_sup):\n",
    "    itemsets = []\n",
    "    # Dictionary: Itemset as key - Support as value\n",
    "    supports = {}\n",
    "    # Dictionary: Itemset as key - Frequency as value\n",
    "    frequencies = {}\n",
    "    transaction_count = len(dataset)\n",
    "\n",
    "    # Find unique values (1-itemsets)\n",
    "    unique_list = unique_values(dataset)\n",
    "\n",
    "    # First step of the algorithm, generation of frequent 1-itemsets\n",
    "    \n",
    "    # Although we do not need to find any combinations for frequent 1-itemsets, the use of the combinations function ensures the itemsets always stay as the type of tuples, required for storing them in dictionaries.\n",
    "    combinations = itr.combinations(unique_list, 1)\n",
    "    itemset = []\n",
    "\n",
    "    for combination in combinations:          \n",
    "        appearance_count = 0\n",
    "\n",
    "        for row in dataset:\n",
    "            # Check if a row of the given dataset contains all values of the combination\n",
    "            if all(value in row for value in combination):\n",
    "                appearance_count = appearance_count + 1\n",
    "\n",
    "        support = appearance_count / transaction_count\n",
    "\n",
    "        # Store the support of the itemset\n",
    "        # We need to store all supports of the 1-itemsets for candidate pruning\n",
    "        supports[combination] = support\n",
    "\n",
    "        # Store where an itemset appears in the dataset\n",
    "        frequencies[combination] = appearance_count\n",
    "\n",
    "        if support >= min_sup:\n",
    "            # Store the 1-frequent itemset\n",
    "            itemset.append(combination)\n",
    "\n",
    "    itemset.sort()\n",
    "    # Store the frequent 1-itemsets\n",
    "    itemsets.append(itemset)\n",
    "\n",
    "    # Second step of the algorithm: repeating until we run out of frequent itemsets\n",
    "\n",
    "    # Algorithm \"k\" variable\n",
    "    # While it should be set to 2 here, we start k at 1 to\n",
    "    # ensure that iterating through a list remains simple.\n",
    "    k = 1\n",
    "\n",
    "    while itemsets[k-1] != []:\n",
    "        # Generate a candidate set\n",
    "        candidate_set = generate_apriori_candidates(itemsets[k-1], k)\n",
    "        # Prune candidate itemsets\n",
    "        pruned_candidate_set = prune_apriori_candidates(candidate_set, supports, min_sup, k)\n",
    "        # Count the support and eliminate infrequent itemsets, returning a new itemset\n",
    "        frequencies, new_itemset, supports = candidate_elimination_apriori(candidate_set, dataset, frequencies, supports, min_sup, k)\n",
    "\n",
    "        itemsets.append(new_itemset)\n",
    "        k = k + 1\n",
    "\n",
    "    return frequencies, itemsets, supports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "frequencies, itemsets, supports = apriori(dataset=data, min_sup=0.1)"
   ]
  },
  {
   "source": [
    "Below are the found itemsets using the apriori algorithm."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--------------------------------------\nPossible 1-itemsets with supports:\n('Bread',) 0.43780935653840014\n('Butter',) 0.4375698547022194\n('Cheese',) 0.43717068497525147\n('Coffee Powder',) 0.4398052051732397\n('Ghee',) 0.43988503911863325\n('Lassi',) 0.43365799137793387\n('Milk',) 0.44116238224493054\n('Panner',) 0.4346159987226569\n('Sugar',) 0.43764968864761294\n('Sweet',) 0.43772952259300657\n('Tea Powder',) 0.4297461280536484\n('Yougurt',) 0.4393262015008782\n--------------------------------------\nPossible 2-itemsets with supports:\n('Bread', 'Butter') 0.1969503432859652\n('Bread', 'Cheese') 0.20197988184576082\n('Bread', 'Coffee Powder') 0.20182021395497365\n('Bread', 'Ghee') 0.19982436532013412\n('Bread', 'Lassi') 0.20006386715631486\n('Bread', 'Milk') 0.20094204055564427\n('Bread', 'Panner') 0.20357656075363245\n('Bread', 'Sugar') 0.19790835063068818\n('Bread', 'Sweet') 0.20269838735430304\n('Bread', 'Tea Powder') 0.1951939964873064\n('Bread', 'Yougurt') 0.20014370110170845\n('Butter', 'Cheese') 0.19838735430304966\n('Butter', 'Coffee Powder') 0.19974453137474055\n('Butter', 'Ghee') 0.20197988184576082\n('Butter', 'Lassi') 0.19966469742934695\n('Butter', 'Milk') 0.19886635797541113\n('Butter', 'Panner') 0.1982276864122625\n('Butter', 'Sugar') 0.20525307360689765\n('Butter', 'Sweet') 0.20301772313587738\n('Butter', 'Tea Powder') 0.19295864601628612\n('Butter', 'Yougurt') 0.20190004790036722\n('Cheese', 'Coffee Powder') 0.20094204055564427\n('Cheese', 'Ghee') 0.1973495130129331\n('Cheese', 'Lassi') 0.1983075203576561\n('Cheese', 'Milk') 0.19703017723135877\n('Cheese', 'Panner') 0.19934536164777264\n('Cheese', 'Sugar') 0.19639150566821012\n('Cheese', 'Sweet') 0.1994251955931662\n('Cheese', 'Tea Powder') 0.19088296343605302\n('Cheese', 'Yougurt') 0.202139549736548\n('Coffee Powder', 'Ghee') 0.20581191122465273\n('Coffee Powder', 'Lassi') 0.20054287082867636\n('Coffee Powder', 'Milk') 0.20102187450103784\n('Coffee Powder', 'Panner') 0.19814785246686892\n('Coffee Powder', 'Sugar') 0.19982436532013412\n('Coffee Powder', 'Sweet') 0.19926552770237904\n('Coffee Powder', 'Tea Powder') 0.1956730001596679\n('Coffee Powder', 'Yougurt') 0.20397573048060036\n('Ghee', 'Lassi') 0.20046303688328276\n('Ghee', 'Milk') 0.20046303688328276\n('Ghee', 'Panner') 0.20142104422800575\n('Ghee', 'Sugar') 0.20086220661025067\n('Ghee', 'Sweet') 0.19990419926552772\n('Ghee', 'Tea Powder') 0.1930384799616797\n('Ghee', 'Yougurt') 0.197748682739901\n('Lassi', 'Milk') 0.20269838735430304\n('Lassi', 'Panner') 0.19934536164777264\n('Lassi', 'Sugar') 0.19982436532013412\n('Lassi', 'Sweet') 0.20565224333386556\n('Lassi', 'Tea Powder') 0.19471499281494492\n('Lassi', 'Yougurt') 0.1968705093405716\n('Milk', 'Panner') 0.1982276864122625\n('Milk', 'Sugar') 0.204614402043749\n('Milk', 'Sweet') 0.20054287082867636\n('Milk', 'Tea Powder') 0.19511416254191283\n('Milk', 'Yougurt') 0.20062270477406993\n('Panner', 'Sugar') 0.19998403321092129\n('Panner', 'Sweet') 0.19998403321092129\n('Panner', 'Tea Powder') 0.19231997445313748\n('Panner', 'Yougurt') 0.1994251955931662\n('Sugar', 'Sweet') 0.19918569375698547\n('Sugar', 'Tea Powder') 0.19742934695832667\n('Sugar', 'Yougurt') 0.20190004790036722\n('Sweet', 'Tea Powder') 0.19982436532013412\n('Sweet', 'Yougurt') 0.19782851668529458\n('Tea Powder', 'Yougurt') 0.19846718824844323\n--------------------------------------\nPossible 3-itemsets with supports:\n('Lassi', 'Panner', 'Sweet') 0.10098994092288041\n--------------------------------------\nNo 4-itemsets have been found.\nTry to lower the minimum support value if that was not intended.\n--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_itemsets(itemsets, supports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = mine_association_rules(data, frequencies, itemsets, supports, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Association rules found:\n['Panner', 'Sweet'] => ['Lassi'] | Support: 0.10098994092288041 | Confidence: 0.5049900199600799\n['Lassi', 'Panner'] => ['Sweet'] | Support: 0.10098994092288041 | Confidence: 0.5066079295154186\n"
     ]
    }
   ],
   "source": [
    "print_association_rules(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP-Growth algorithm\n",
    "This implementation is NOT finished, therefore it will NOT be taken upon further evaluation, although it does have the grounds for further development, such as the creation of the Node class and successful pass of the first step of the algorithm (FP-Tree construction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    item_name = None\n",
    "    parent = None\n",
    "    children = []\n",
    "    linked_node = None\n",
    "    count = 1\n",
    "\n",
    "    def __init__(self, item_name, parent):\n",
    "        self.item_name = item_name\n",
    "        self.parent = parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp_growth(dataset, min_sup):\n",
    "    itemsets = []\n",
    "    # Dictionary: Itemset as key - Support as value\n",
    "    supports = {}\n",
    "    # Dictionary: Itemset as key - Frequency as value\n",
    "    frequencies = {}\n",
    "    transaction_count = len(dataset)\n",
    "\n",
    "    # Find unique values (1-itemsets)\n",
    "    unique_list = unique_values(dataset)\n",
    "\n",
    "    # First step of the algorithm, generation of frequent 1-itemsets\n",
    "    # First pass over the dataset\n",
    "    \n",
    "    # Although we do not need to find any combinations for frequent 1-itemsets, the use of the combinations function ensures the itemsets always stay as the type of tuples, required for storing them in dictionaries.\n",
    "    combinations = itr.combinations(unique_list, 1)\n",
    "    itemset = []\n",
    "\n",
    "    for combination in combinations:          \n",
    "        appearance_count = 0\n",
    "\n",
    "        for row in dataset:\n",
    "            # Check if a row of the given dataset contains all values of the combination\n",
    "            if all(value in row for value in combination):\n",
    "                appearance_count = appearance_count + 1\n",
    "\n",
    "        support = appearance_count / transaction_count\n",
    "\n",
    "        # Store the support of the itemset\n",
    "        supports[combination] = support\n",
    "\n",
    "        if support >= min_sup:\n",
    "            # Store where an itemset appears in the dataset\n",
    "            frequencies[combination] = appearance_count\n",
    "\n",
    "    # Sort (descending) the frequent 1-itemsets according to frequency\n",
    "    sorted_frequencies = sorted(frequencies.items(), key=lambda i: i[1], reverse=True)\n",
    "\n",
    "    itemset = []\n",
    "    for i in sorted_frequencies:\n",
    "        itemset.append(i[0])\n",
    "\n",
    "    # Pass 2 of the dataset\n",
    "    # We create the FP-Tree here\n",
    "    fp_tree = [[] for _ in range(len(itemset))]\n",
    "\n",
    "    for row in dataset:\n",
    "        previous_nodes = []\n",
    "        for item in itemset:\n",
    "            # Check if a row of the given dataset contains the 1-itemset\n",
    "            if all(value in tuple(row) for value in item):\n",
    "                # Check if there are no parents, meaning we start at root\n",
    "                if not previous_nodes:\n",
    "                    # Create a node with \"null\" root being the parent\n",
    "                    node = Node(item, \"null\")\n",
    "                    previous_nodes.append(node)\n",
    "                else:\n",
    "                    # We already have previously found nodes with higher frequencies\n",
    "                    # We need to update the child of the last node\n",
    "                    node = Node(item, previous_nodes[-1])\n",
    "                    previous_nodes[-1].child = node\n",
    "                    previous_nodes.append(node)\n",
    "            \n",
    "        # Compare here the previous nodes with the FP-tree and add counts or create new nodes\n",
    "        if previous_nodes:\n",
    "            count = 0\n",
    "            for depth_level in previous_nodes:\n",
    "                node_not_existing_yet = True\n",
    "                if fp_tree[count]:\n",
    "                    for node in fp_tree[count]:\n",
    "                        if depth_level.item_name == node.item_name:\n",
    "                            node.count = node.count + 1\n",
    "                            \n",
    "                            if depth_level.children != node.children:\n",
    "                                node.children.append(depth_level.children)\n",
    "\n",
    "                            node_not_existing_yet = False\n",
    "                            break\n",
    "                    if node_not_existing_yet == True:\n",
    "                        fp_tree[count].append(depth_level)\n",
    "\n",
    "                else:\n",
    "                    fp_tree[count].append(depth_level)\n",
    "                count = count + 1\n",
    "\n",
    "    \n",
    "    # We need to find head pointers for linking nodes.\n",
    "    header_table = {}\n",
    "\n",
    "    for item in itemset:\n",
    "        header_pointer_found = False\n",
    "        while header_pointer_found == False:\n",
    "            for column in range(len(fp_tree)-1, -1, -1):\n",
    "                for node in fp_tree[column]:\n",
    "                    if node.item_name == item:\n",
    "                        # If we do not have a head pointer yet for this item, set the deepest found node as it\n",
    "                        if header_pointer_found == False:\n",
    "                            header_table[item] = node\n",
    "                            header_pointer_found = True\n",
    "                        # If we do have a set head pointer already for this item, use this node as a linked node\n",
    "                        else:\n",
    "                            main_node = header_table[item]\n",
    "                            linked_node_set = False\n",
    "                            temp_node = main_node.linked_node\n",
    "\n",
    "                            while linked_node_set == False:\n",
    "                                if temp_node is not None:\n",
    "                                    temp_node = temp_node.linked_node\n",
    "                                else:\n",
    "                                    temp_node = node\n",
    "                                    linked_node_set = True\n",
    "                                    \n",
    "                            \n",
    "    return fp_tree\n",
    "        # Pass 2 finished, we have a FP-Tree\n",
    "        # Lacking - Step 2: Frequent Itemset Generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fp_tree = fp_growth(dataset=data, min_sup=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment on the dataset, compare run-time performance\n",
    "Below we will run both brute-force and apriori algorithms on minimum support values of 0.4, 0.2, 0.1 and 0.05, with a minimum confidence value of 0.5. The brute-force algorithm will be limited at 8-itemsets, as it is expected that with a higher itemset limit, the runtime will grow massively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_times(minimum_support, minimum_confidence, itemset_limit_value):\n",
    "    start = timer()\n",
    "    frequencies, itemsets, supports = frequent_itemset(dataset=data, min_sup=minimum_support, itemset_limit=itemset_limit_value)\n",
    "    bruteforce_output = mine_association_rules(data, frequencies, itemsets, supports, minimum_confidence)\n",
    "    end = timer()\n",
    "    bruteforce_time = end - start\n",
    "\n",
    "    start = timer()\n",
    "    frequencies, itemsets, supports = apriori(dataset=data, min_sup=minimum_support)\n",
    "    apriori_output = mine_association_rules(data, frequencies, itemsets, supports, minimum_confidence)\n",
    "    end = timer()\n",
    "    apriori_time = end - start\n",
    "\n",
    "    results = []\n",
    "    results.append(\"------------------------------------------------------------\")\n",
    "    results.append(\"Evaluating times for apriori and bruteforce algorithms:\")\n",
    "    results.append(\"Minimum support: {} ||| Minimum confidence: {} ||| Itemset limit for bruteforce: {}\".format(minimum_support, minimum_confidence, itemset_limit_value))\n",
    "    results.append(\"\")\n",
    "    results.append(\"Time taken for the bruteforce algorithm: {} seconds\".format(bruteforce_time))\n",
    "    if not bruteforce_output[0]:\n",
    "        results.append(\"No association rules have been found.\")\n",
    "    else:\n",
    "        results.append(\"Association rules found:\")\n",
    "        for row in range(len(bruteforce_output[0])):\n",
    "            results.append(\"{} => {} | Support: {} | Confidence: {}\".format(bruteforce_output[0][row], bruteforce_output[1][row], bruteforce_output[2][row], bruteforce_output[3][row]))\n",
    "    results.append(\"\")\n",
    "    results.append(\"Time taken for the apriori algorithm: {} seconds\".format(apriori_time))\n",
    "    if not apriori_output[0]:\n",
    "        results.append(\"No association rules have been found.\")\n",
    "    else:\n",
    "        results.append(\"Association rules found:\")\n",
    "        for row in range(len(apriori_output[0])):\n",
    "            results.append(\"{} => {} | Support: {} | Confidence: {}\".format(apriori_output[0][row], apriori_output[1][row], apriori_output[2][row], apriori_output[3][row]))\n",
    "    results.append(\"\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------------------------------------------------------------\nEvaluating times for apriori and bruteforce algorithms:\nMinimum support: 0.4 ||| Minimum confidence: 0.5 ||| Itemset limit for bruteforce: 8\n\nTime taken for the bruteforce algorithm: 71.40586452599973 seconds\nNo association rules have been found.\n\nTime taken for the apriori algorithm: 1.4229402240002855 seconds\nNo association rules have been found.\n\n------------------------------------------------------------\nEvaluating times for apriori and bruteforce algorithms:\nMinimum support: 0.2 ||| Minimum confidence: 0.5 ||| Itemset limit for bruteforce: 8\n\nTime taken for the bruteforce algorithm: 76.0077230689999 seconds\nNo association rules have been found.\n\nTime taken for the apriori algorithm: 2.4180687809998744 seconds\nNo association rules have been found.\n\n------------------------------------------------------------\nEvaluating times for apriori and bruteforce algorithms:\nMinimum support: 0.1 ||| Minimum confidence: 0.5 ||| Itemset limit for bruteforce: 8\n\nTime taken for the bruteforce algorithm: 72.6027445210002 seconds\nAssociation rules found:\n['Panner', 'Sweet'] => ['Lassi'] | Support: 0.10098994092288041 | Confidence: 0.5049900199600799\n['Lassi', 'Panner'] => ['Sweet'] | Support: 0.10098994092288041 | Confidence: 0.5066079295154186\n\nTime taken for the apriori algorithm: 9.321411819999867 seconds\nAssociation rules found:\n['Panner', 'Sweet'] => ['Lassi'] | Support: 0.10098994092288041 | Confidence: 0.5049900199600799\n['Lassi', 'Panner'] => ['Sweet'] | Support: 0.10098994092288041 | Confidence: 0.5066079295154186\n\n------------------------------------------------------------\nEvaluating times for apriori and bruteforce algorithms:\nMinimum support: 0.05 ||| Minimum confidence: 0.5 ||| Itemset limit for bruteforce: 8\n\nTime taken for the bruteforce algorithm: 80.58706324400009 seconds\nAssociation rules found:\n['Sugar', 'Sweet'] => ['Butter'] | Support: 0.0997125977965831 | Confidence: 0.5006012024048097\n['Panner', 'Sweet'] => ['Lassi'] | Support: 0.10098994092288041 | Confidence: 0.5049900199600799\n['Lassi', 'Panner'] => ['Sweet'] | Support: 0.10098994092288041 | Confidence: 0.5066079295154186\n\nTime taken for the apriori algorithm: 19.516647481999826 seconds\nAssociation rules found:\n['Sugar', 'Sweet'] => ['Butter'] | Support: 0.0997125977965831 | Confidence: 0.5006012024048097\n['Panner', 'Sweet'] => ['Lassi'] | Support: 0.10098994092288041 | Confidence: 0.5049900199600799\n['Lassi', 'Panner'] => ['Sweet'] | Support: 0.10098994092288041 | Confidence: 0.5066079295154186\n\n"
     ]
    }
   ],
   "source": [
    "results = [[], [], [], [], []]\n",
    "\n",
    "results[0] = compare_times(0.4, 0.5, 8)\n",
    "results[1] = compare_times(0.2, 0.5, 8)\n",
    "results[2] = compare_times(0.1, 0.5, 8)\n",
    "results[3] = compare_times(0.05, 0.5, 8)\n",
    "\n",
    "file_ready_results = []\n",
    "\n",
    "for column in results:\n",
    "    for row in column:\n",
    "        print(row)\n",
    "        file_ready_results.append(row)"
   ]
  },
  {
   "source": [
    "### Interesting association rules\n",
    "As we can see above, there is a small amount of association rules found.\n",
    "\n",
    "Customers containing Sugar and Sweet tend to buy Butter, while transactions containing Panner and Sweet tend to have Lassi as well. The latter is vice versa as well - transactions containing Lassi and Panner show a tendency to have Sweet in the transaction. \n",
    "\n",
    "In terms of evaluation, both algorithms are accurate in finding association rules.\n",
    "\n",
    "### Performance evaluation\n",
    "The a priori algorithm beats the brute-force in terms of time efficiency at each trial (0.4, 0.2, 0.1, 0.05 minimum support). \n",
    "\n",
    "However, while the time taken for the brute-force algorithm remains relatively constant, as it solely depends on the itemset limit due to treating all itemsets as possible candidate rules, there is a clear scalability issue shown by the a priori algorithm (1.42s -> 2.41s -> 9.32s -> 19.51s), which is due to the fact that the a priori algorithm has to traverse through the dataset each time in its main loop. Since the number of the potential candidate rules rises when the minimum support value lowers, we can see how time complex this algorithm is. A potential solution would be to use algorithms that traverse through the dataset as little as possible, such as the FP-Growth algorithm."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_results(file_ready_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}